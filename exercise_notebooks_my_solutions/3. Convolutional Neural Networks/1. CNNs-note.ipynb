{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutional Neural Networks\n",
    "\n",
    "\n",
    "## Lesson 1: Convolutioanl Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a convolutional layer + a pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we define a custom MLPs, we need to define 2 parts:\n",
    "\n",
    "1. a convolutional layer\n",
    "2. a feedforward behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__();\n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "1. `kernel_size` can be a scalar or a tuple, e.g. `kernel_size=3` === `kernel_size=(3, 3)`\n",
    "2. `stride` can be a scalar or a tuple too\n",
    "\n",
    "[torch.nn.Conv2d doc](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
    "\n",
    "[torch.nn.MaxPool2d doc](https://pytorch.org/docs/stable/nn.html#maxpool2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sequential Models\n",
    "\n",
    "We can also create a CNN using a `Sequential` in `__init__` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 2, stride=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(true),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(true)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate number of parameters in a CNN**\n",
    "\n",
    "Let:\n",
    "\n",
    "1. `K` : number of filters\n",
    "2. `F` : kernel size (assume kernel is square)\n",
    "3. `D_in` : input depth\n",
    "\n",
    "Then:\n",
    "\n",
    "1. For each filter, the number of its parameters is `F*F*D_in`\n",
    "2. For *K* filters, the number of their parameters is `K*F*F*D_in`\n",
    "3. For the CNN, each filter should work with a bias, so the total number of parameters is `K*F*F*D_in + K`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the shape of a CNN output**\n",
    "\n",
    "Let:\n",
    "\n",
    "1. `K` : number of filters\n",
    "2. `F` : kernel size\n",
    "3. `S` : kernel stride (step size)\n",
    "4. `P` : padding for the input\n",
    "5. `W_in` : input size (assume output of previous layer is a square)\n",
    "\n",
    "The spatial dimensions of the CNN layer is **$\\frac{W\\_in - F + 2*P}{S} + 1$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Flattening\n",
    "\n",
    "To finish all of convolutional layer parts, at the end of the CNN kingdom, we need to flatten the output of the last convolutional layer, so that all parameters can be seen as a vector passing to the next kingdom (typically is MLPs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quiz\n",
    "\n",
    "1. Quiz on depth\n",
    "2. Quiz on output size\n",
    "3. Quiz on number of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Quiz on output size\n",
    "class cnn3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 130, 130])\n",
      "tensor([[[[-1.8209e+00, -2.1117e+00, -1.5673e+00,  ...,  1.0953e-01,\n",
      "            2.1953e-01, -9.2199e-01],\n",
      "          [-1.9189e+00, -4.6497e-01, -2.0496e-01,  ..., -1.1678e+00,\n",
      "           -7.7826e-01, -5.4527e-02],\n",
      "          [-2.2725e-01,  4.9749e-01,  1.9517e-03,  ...,  2.4311e-01,\n",
      "           -9.0033e-02,  7.9273e-01],\n",
      "          ...,\n",
      "          [ 2.1072e-01, -1.2518e+00,  1.6175e+00,  ...,  7.2644e-01,\n",
      "            6.7363e-01, -2.0206e+00],\n",
      "          [ 6.7155e-01,  5.9673e-01,  7.1884e-01,  ...,  8.4635e-01,\n",
      "            8.0099e-01, -1.3762e+00],\n",
      "          [ 4.9792e-01,  1.0314e+00, -2.3386e-01,  ..., -1.9264e+00,\n",
      "            8.6474e-01, -5.2759e-01]],\n",
      "\n",
      "         [[-4.0858e-01, -1.8259e+00, -1.6555e-01,  ..., -3.1100e-01,\n",
      "            4.8991e-01, -9.1660e-01],\n",
      "          [-3.6151e-01, -2.2194e-01, -1.3652e+00,  ..., -8.9918e-01,\n",
      "            3.2796e-01, -6.7639e-01],\n",
      "          [-1.1627e+00, -9.0478e-01, -4.8279e-02,  ..., -1.4363e+00,\n",
      "           -8.5079e-01,  1.7682e-01],\n",
      "          ...,\n",
      "          [-7.5038e-02, -3.7234e-01, -9.3438e-01,  ..., -9.9265e-01,\n",
      "           -1.0602e+00,  4.6271e-01],\n",
      "          [-1.1195e-01, -1.2958e+00, -1.8961e-01,  ..., -1.3135e+00,\n",
      "            2.3498e-01,  4.4170e-02],\n",
      "          [-2.1624e+00, -6.2153e-01, -6.4192e-01,  ..., -4.6998e-01,\n",
      "            5.5602e-01, -1.2128e+00]],\n",
      "\n",
      "         [[-2.6962e-01,  4.8870e-01, -5.6451e-01,  ..., -9.0463e-01,\n",
      "           -6.7295e-01, -4.1975e-01],\n",
      "          [-1.4934e+00, -1.3123e+00, -7.1304e-01,  ..., -1.3701e+00,\n",
      "            4.0262e-01, -3.2940e-01],\n",
      "          [-6.3829e-01, -1.2779e-02,  7.5686e-03,  ...,  1.1668e+00,\n",
      "           -9.9115e-01, -6.5250e-01],\n",
      "          ...,\n",
      "          [-1.3991e+00,  6.7971e-01, -1.2647e-01,  ..., -1.4766e+00,\n",
      "            1.5751e+00,  7.8291e-02],\n",
      "          [ 1.2414e+00,  1.1352e+00,  9.2092e-02,  ...,  8.6485e-02,\n",
      "           -5.8183e-01,  9.1778e-01],\n",
      "          [ 1.2209e-01, -1.1119e+00, -1.7220e+00,  ...,  4.2725e-01,\n",
      "            2.0900e+00, -5.6333e-01]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cnn3(\n",
       "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# input image is (130, 130, 3)\n",
    "input_img = np.random.randn(1, 3, 130, 130)\n",
    "input_tensor = torch.from_numpy(input_img).float()\n",
    "print(input_tensor.shape)\n",
    "print(input_tensor)\n",
    "\n",
    "# define the cnn3\n",
    "c3 = cnn3()\n",
    "c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "According to the [post](https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but) and [torch.nn.Conv2d doc](https://pytorch.org/docs/stable/nn.html#conv2d), `nn.Conv2d` expects input size $(N, C, H, W)$, output $(N, C, H_{out}, W_{out})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# feed-forward\n",
    "output = c3(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 10, 128, 128]             280\n",
      "         MaxPool2d-2           [-1, 10, 32, 32]               0\n",
      "            Conv2d-3           [-1, 20, 32, 32]           5,020\n",
      "         MaxPool2d-4           [-1, 20, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 5,300\n",
      "Trainable params: 5,300\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 1.52\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 1.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quiz 3: check total number of parameters in cnn3\n",
    "from torchsummary import summary\n",
    "summary(c3, input_size=(3, 130, 130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "Install torchsummary: Run ```pip install torchsummary```\n",
    "\n",
    "[github repo](https://github.com/sksq96/pytorch-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udacity-dl] *",
   "language": "python",
   "name": "conda-env-udacity-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
